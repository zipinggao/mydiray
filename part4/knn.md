## k邻近

###算法介绍

k近邻法(k-nearest neighbor, k-NN)是一种基本的分类与回归方法。输入为：实例的特征向量、对应于在特征空间中的点，输出为实例的类别（多类）。

**实例**：

![](/assets/knn.jpg)

如上图所示，蓝色正方形表示一个类别，红色三角形表示另一个类别，绿色圆圈表示待分类的样本。按照KNN算法，首先我们给k一个值，假设为5，那么如图所示，与绿色圆圈距离最近的5个样本都在虚线圆之内，这五个样本中数量最多的为蓝色正方形所表示的类别，此时绿色圆圈的类别与蓝色正方形相同。同理，假设k为3，此时实线圆之内数量最多的为红色三角形，那么绿色圆圈的类别就与红色三角形的类别相同。

**算法分析**

![](/assets/K邻近_算法.png)

K近邻没有显示的学习过程，也不需要对数据进行学习。

###模型三要素

模型由三个基本要素 —— 距离度量、 k值的选择和分类决策规则决定。

k近邻法中，当训练集，距离度量（如欧氏距离）、k值及分类决策规则（如多数表决）确定后，对于任何一个新的输入实例，它所属的类唯一确定。

**距离度量**

特征空间中两个实例点的距离是两个实例点相似程度的反映。k近邻模型的特征空间一般是n维实数向量空间 ，使用的距离是一般是欧式距离，也可以是其他距离。

![](/assets/knn_k值.png)

***由不同的距离度量所确定的最近邻点是不同的。*** k=1时为曼哈顿距离，k=2时为欧式距离。

**K值选择**

k值的大小决定了邻域的大小。

k值较小：对近邻点敏感，如果近邻点恰是噪声，则预测出错。话句话说，***k值的减小整体模型变得复杂，容易发生过拟合***。

k值较大：容易欠拟合。

应用中，k值一般选较小的数值，然后采用交叉验证法选取最优的k值。

**分类决策规则**

分类规则往往是多数表决，即由输入实例的k个近邻的训练实例中的多数类决定输入的实例。***多数表决规则等价于经验风险最小化。***

**kd树**

参考
[kd树介绍](https://zhuanlan.zhihu.com/p/23966698)