## 朴素贝叶斯

###使用
假设有一手写数据集样本，共有100条记录，记录0-10是10个人各自写的0，记录11-20是10个人各自写的1,……..以此类推一共100条记录。那么这时候外头有个人进来写了个数字X，怎么识别出来它写的几呢？没学习过机器学习的人可能也能提出这样一种方法：我们只要把写的那个数字和0-9进行匹配，那个匹配度最高就是哪个数啦。没错，朴素贝叶斯用的就是这样朴素的思想（开玩笑，这里的朴素可不是这个意思）。

###算法介绍

朴素贝叶斯是基于贝叶斯定理与特征条件独立性假设的分类方法。对于给定的数据，基于特征条件独立性假设分别学习（联合概率分布）先验概率和条件概率，基于学习到的条件概率分布，对输入的$$x$$，求后验概率最大的输出$$y$$。

![](/assets/beyes1.png) 


也就是说，只要我们知道$$P(X = x | Y= c_k),P(Y = c_k)$$和$$P(X= x)$$,就可以求出$$P(Y = c_k | X = x)$$的值。

朴素贝叶斯法的条件独立性假设：

![](/assets/beiyesi2.png)

每个类别的样本的各个特征之间是相互独立的，因此在计算的时候我们就可以将其拆成连乘的形式。由于这是一个较强的假设，朴素贝叶斯算法也由此得名。

**接下来求三个概率值：** $$P(X=x|Y=C_k)$$ ，$$P(Y=C_k)$$以及$$P(X = x)$$

(1)求$$P(X=x|Y=C_k)$$

![](/assets/beiyes3.png)

（2）求$$P(Y=C_k)$$

假设在训练集T中，类别为$$C_K$$的样本个数为$$N_k$$,在类别$$C_k$$的所有样本中$$X^{(j)} = x^{j}$$的样本个数为$$n_{jk}$$,那么:

![](/assets/beiyes4.png)

显而易见的$$P(X= x) = \frac{N_k}{N}$$,$$N$$为样本总数。

（3）求$$P(X = x)$$

![](/assets/beiyes5.png)

注意：当给定一个输入$$x$$,对于不同类别$$C_k$$,$$P(X = x)$$的值是相同，也就是说它的值不会影响我们对于输入实例属于某个类别的判定，因此我们只需设定：

$$P(X = x) = 1$$

**最终**:

![](/assets/beiyes6.png)


**综合理解**

![](/assets/beiye7.png)

往下转换：

![](/assets/beiyes8.png)

把$$P(X=x|Y=C_k)$$这一项变成了连乘，至于为什么能连乘，下图有详细说明:
![](/assets/beiye9.png)

比较$$Y$$为不同$$C_k$$的情况下哪个概率最大，那就表示属于哪个类的可能性最大。所以前头式子前头加上一个$$argmax$$，表示求让后式值最大的$$Ck$$。

![](/assets/beiys11.png)

上图中圈出来这一项是在$$Y$$为不同$$C_k$$情况下的连乘，所以不管$$k$$为多少，所有$$C_k$$连乘结果肯定是一致的，在比较谁的值最大时，式子里面的常数无法对结果的大小造成影响，可以去掉，结果如下：

![](/assets/beiye12.png)

###极大似然估计求参数

![](/assets/beiyes13.png)

$$N$$为训练样本的数目，假设有100个样本，那$$N$$就是100。分子中$$I$$是指示函数，括号内条件为真时指示函数为1，反之为0。分子啥意思呢？就是说对于$$C_k$$，在这100个样本离有多少是$$C_k$$分类的，就比如$$C_k$$为$$C_1$$时表示数字类别1，这时候就是看这100个样本里有多少个数字1，处于总的100，就是类别1的概率，也就是$$P(Y=C_1)$$。简单不？相当简单。我们再看第二项。

![](/assets/beisi15.png)

原理和上面第一项的一样，也是通过指示函数来计数得出概率

###朴素贝叶斯算法

![](/assets/beiyes16.png)

首先根据给定的训练样本求先验概率和条件概率：

![](/assets/beiyesi17.png)

![](/assets/beiyesi18.png)

###贝叶斯估计

那么多概率连乘，如果其中有一个概率为0怎么办？那整个式子直接就是0了，这样不对。所以我们连乘中的每一项都得想办法让它保证不是0，哪怕它原先是0,（如果原先是0，表示在所有连乘项中它概率最小，那么转换完以后只要仍然保证它的值最小，对于结果的大小来说没有影响。）。这里就使用到了贝叶斯估计。

![](/assets/beiyesi19.png)

它通过分母和分子各加上一个数来保证始终不为0.为什么要分母加$$S_j$$或者$$K$$倍$$λ$$，以及分子要加个1倍$$λ$$，读者可以试验一下，这样处理以后，所有概率的总和仍然是1，所以为什么这么加，只不过这么加完以后，概率仍然是概率，总和仍然为1，但所有项都大于0.

![](/assets/朴素贝叶斯_拉普拉斯平滑.png)

**补充**
使用贝叶斯估计虽然保证了所有连乘项的概率都大于0，不会再出现某一项为0结果为0的情况。但若一个样本数据时高维的，比如说100维（100其实并不高），连乘项都是0-1之间的，那100个0-1之间的数相乘，最后的数一定是非常非常小了，可能无限接近于0。对于程序而言过于接近0的数可能会造成下溢出，也就是精度不够表达了。所以我们会给整个连乘项取对数，这样哪怕所有连乘最后结果无限接近0，那取完log以后数也会变得很大（虽然是负的很大），计算机就可以表示了。同样，多项连乘取对数，对数的连乘可以表示成对数的相加，在计算上也简便了。所以在实际运用中，不光需要使用贝叶斯估计（保证概率不为0），同时也要取对数（保证连乘结果不下溢出）。

有人可能关心取完log以后结果会不会发生变化，答案是不会的。log在定义域内是递增函数，log(x)中的x也是递增函数（x在x轴的最大处，x值也就越大，有些拗口，可以仔细想想）。在单调性相同的情况下，连乘得到的结果大，log取完也同样大，并不影响不同的连乘结果的大小的比较。好啦，上代码。