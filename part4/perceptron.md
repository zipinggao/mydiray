## 感知机

###模型介绍

**目的**：感知机学习旨在求出对训练数据进行线性划分的超平面。

**定义**：输入空间（特征空间）是$$X \subseteq \mathbb{R}^n$$,输出空间$$Y={+1,-1}$$,输入$$x \in X$$的特征向量，对应于输入空间中的点：输出$$y \in Y$$,由输入空间到输出空间有函数：

$$f(x) = sign(w * x) +b$$

则$$f(x)$$称为感知机，$$w$$和$$b$$为感知机的模型参数，$$w \in \mathbb{R}^n$$,叫权值。$$b \in \mathbb{R}$$ 叫做偏执。$$w \cdot x$$表示内积。$$sign（\cdot）$$是符号函数：

![](/assets/sign.png)

当$$x$$大于等于0，$$sign$$输出1，否则输出-1。那么往前递归一下，$$wx+b$$如果大于等于0，$$f(x)$$就等于1，反之$$f(x)$$等于-1。

**直观理解**

![](/assets/感知机_划分超平面.png)

有坐标轴（图中黑色线）横的为$$x_1$$轴，竖的$$x_2$$轴,坐标系中没一点都有$$(x_1 ,  x_2)$$决定。如果我们将这张图应用在判断零件是否合格上，$$x_1$$表示零件长度，$$x_2$$表示零件质量，坐标轴表示零件的均值长度和均值重量，并且蓝色的为合格产品，黄色为劣质产品，需要剔除。那么很显然如果零件的长度和重量都大于均值，说明这个零件是合格的。也就是在第一象限的所有蓝色点。反之如果两项都小于均值，就是劣质的，比如在第三象限的黄色点。

预测上，一个新样本，长度x1，质量x2，如果两项都大于均值，说明零件合格。这就是我们人的人工智能。

程序拿到手的是当前图里所有点的信息以及标签，也就是说它知道所有样本$$x的$$坐标为$$（x_1， x_2）$$，同时它属于蓝色或黄色。对于目前手里的这些点，要是能找到一条直线把它们分开就好了，这样我拿到一个新的零件，知道了它的质量和重量，我就可以判断它在线的哪一侧，就可以知道它可能属于好的或坏的零件了。例如图里的黄、蓝、粉三条线，都可以完美地把当前的两种情况划分开。甚至$$x_1$$坐标轴或$$x_2$$坐标轴都能成为一个划分直线（这两个直线均能把所有点正确地分开）。

**结论**
    
如果一条直线能够不分错一个点，那就是一条好的直线

$$wx+b$$ 最优直线,二维情况下：$$y=ax+b$$。在二维中，$$w$$就是$$a$$，$$b$$还是$$b$$。所以$$wx+b$$是一条直线（比如说本文最开始那张图中的蓝线）.新的点$$x$$在蓝线左侧，那么$$wx+b<0$$,再经过$$sign$$，最后$$f$$输出-1，如果在右侧，输出1。**但是:** $$y=ax+b$$，只要点在$$x$$轴上方，甭管点在线的左侧右侧，最后结果都是大于0啊，这个值得正负跟线有啥关系？其实$$wx+b$$和$$ax+b$$表现直线的形式一样，但是又稍有差别。我们把最前头的图逆时针旋转45度，蓝线是不是变成x轴了？哈哈这样是不是原先蓝线的右侧变成了x轴的上方了？其实感知机在计算$$wx+b$$这条线的时候，已经在暗地里进行了转换，使得用于划分的直线变成$$x$$轴，左右侧分别为$$x$$轴的上方和下方，也就成了正和负。

在本文中使用零件作为例子，上文使用了长度和重量$$（x1，x2）$$来表示一个零件的属性，所以一个二维平面就足够，那么如果零件的品质和色泽也有关系呢？那就得加一个$$x3$$表示色泽，样本的属性就变成了$$（x1，x2，x3）$$，变成三维了。$$wx+b$$并不是只用于二维情况，在三维这种情况下，仍然可以使用这个公式。所以$$wx+b$$与$$ax+b$$只是在二维上近似一致，实际上是不同的东西。在三维中$$wx+b$$是啥？我们想象屋子里一个角落有蓝点，一个角落有黄点，还用一条直线的话，显然是不够的，需要一个平面！所以在三维中，$$wx+b$$是一个平面！。四维呢？emmm…好像没法描述是个什么东西可以把四维空间分开，但是对于四维来说，应该会存在一个东西像一把刀一样把四维空间切成两半。能切成两半，应该是一个对于四维来说是个平面的东西，就像对于三维来说切割它的是一个二维的平面，二维来说是一个一维的平面。总之四维中$$wx+b$$可以表示为一个相对于四维来说是个平面的东西，然后把四维空间一切为二，我们给它取名叫超平面。由此引申，在高维空间中，$$wx+b$$是一个划分超平面，这也就是它正式的名字。

##数学知识##
**法向量**垂直于平面的直线所表示的向量为该平面的法向量，如果一个非零向量$$n$$与平面$$a$$垂直，则称向量$$n$$为平面$$a$$的法向量。用方程$$ax+by+cz=d$$表示的平面，向量$$(a,b,c)$$就是其法线,$$d$$是截距。

对于：

$$wx+b=0(w = w_1 ,w_2 ,,,,,,w-n)（x=x_1,x_2,,,,,,x_n）$$

是一个$$n$$维空间中的超平面$$S$$，其中$$w$$是超平面的法向量，$$b$$是超平面的截距，这个超平面将特征空间划分成两部分，位于两部分的点分别被分为正负两类，所以，超平面$$S$$称为分离超平面。

**点到直线距离**

首先，点$$P（x_0,y_0）$$到直线的距离公式：

$$d = \frac{Ax_0 + By_0 + C}{\sqrt{A^2 + B^2}}$$

类似的，设置超平面公式为$$h = wx + b$$，其中$$(w = w_1 ,w_2 ,,,,,,w-n)（x=x_1,x_2,,,,,,x_n）$$，这样样本点$$x_i$$到超平面的距离为：

$$d = \frac{|w \cdot x_i + b|}{||w||}$$

二维空间中,所有点到x轴的距离其实就是$$wx+b$$的值,考虑到$$x$$轴下方的点，得加上绝对值$$|wx+b|$$,求所有误分类点的距离和，也就是求$$|wx+b|$$的总和.让$$|wx+b|$$最小化。很简单啊，把w和b等比例缩小就好啦，比如说w改为0.5w，b改为0.5b，线还是那条线，但是值缩小两倍啦！你还不满意？我可以接着缩！缩到0去！所以啊，我们要加点约束，让整个式子除以w的模长。啥意思？就是w不管怎么样，要除以它的单位长度。如果我w和b等比例缩小，那||w||也会等比例缩小，值一动不动，很稳。没有除以模长之前，|wx+b|叫函数间隔，除模长之后叫几何间隔，几何间隔可以认为是物理意义上的实际长度，管你怎么放大缩小，你物理距离就那样，不可能改个数就变。在机器学习中求距离时，通常是使用几何间隔的，否则无法求出解。


**学习策略**

![](/assets/感知机_学习策略.png)


![](/assets/感知机_损失函数.png)

对于正确分类的数据：$$y_i$$和$$wx+b$$为同号，当分类错误时异号，加上“-”结果就是正。除以$$w的$$模长，就是单个误分类的点到超平面的距离。距离总和就是所有误分类的点相加。

上图最后说不考虑除以模长，就变成了函数间隔：感知机是误分类驱动的算法，它的终极目标是没有误分类的点，如果没有误分类的点，总和距离就变成了0，w和b值怎样都没用。所以几何间隔和函数间隔在感知机的应用上没有差别，为了计算简单，使用函数间隔。

![](/assets/感知机模长.jpg)

![](/assets/感知机_损失函数正式定义.png)

###学习算法

![](/assets/感知机_学习算法.png)

$$w = w + \eta{\frac{\partial {y_i(wx_i + b)}}{\partial w}} = w + \eta y_i x_i$$
$$b = b + \eta{\frac{\partial {y_i(wx_i + b)}}{\partial b}} = b + \eta y_i$$

感知机使用梯度下降方法求得w和b的最优解。

这里给出书中的一个例子:

![](/assets/感知机例子1.png)
![](/assets/感知机例子2.png)
![](/assets/感知机例子3.png)

###感知机对偶形式

有$$w,b的跟新公式$$：

$$w =  = w + \eta y_i x_i$$

$$b =b + \eta y_i$$


当$$w_0 =0 ,b_0 = 0$$时,经过$$n$$次修改后，令$$\alpha =n \eta $$：

$$w =\sum_{i = 0}^n \eta y_i x_i = \sum_{i = 0}^n \alpha_{i} y_i x_i$$,

$$b =\sum_{i = 0}^n \eta y_i  = \sum_{i = 0}^n \alpha_{i} y_i$$,

而对偶形式的基本思想是将w和b表示成x和y的线性组合形式，从而求出w和b。初始$$\alpha =-0 $$。

###原始形式与对偶形式的选择

1.在向量维数（特征数）过高时，计算内积非常耗时，应选择对偶形式算法加速。

2.在向量个数（样本数）过多时，每次计算累计和就没有必要，应选择原始算法。
